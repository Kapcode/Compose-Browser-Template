<!DOCTYPE html>
<html lang="en" style="height: 100%;">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ethics</title>
    <link rel="stylesheet" href="styles.css">

    <style>
        .first-line-indented-text {
          text-indent: 30px; /* This creates the "tab" space */
        }
    </style>



</head>
<body style="height: 100%;">
<div class="ethics-box">



    <div class="content-overlay-box">




            <h1 style="display: flex; align-items: center; justify-content: center;">
                <img src="images/x.png" alt="ximage" style="height: 1em; width: auto; margin-right: 0.5em;">
                Ethics, AI, and LLMs
                <img src="images/check.png" alt="checkimage" style="height: 1em; width: auto; margin-left: 0.5em;">
            </h1>



        <h2>What The Top LLMs have to say on this topic as a whole</h2>
        <p><strong>Why am I including so much ai generated content bellow?</strong></p>
        <p>To illustrate the current state (which i believe to be very good), of the most popular LLMs and how "self-aware-like" they are about all of ethics situations surrounding them. (self informed, not human-like, based on training data)</p>






        <div class="image-expander">
            <input type="checkbox" id="expander2-checkbox" class="image-expander-checkbox">
            <label for="expander2-checkbox" class="image-expander-label">
                <img src="images/stressed-breath.gif" alt="Click to expand more about another topic" width="143" height="64">
                <span>Hallucination Example: What gemini has to say about it?</span><!-- You can have just the image in the label -->
            </label>
            <div class="image-expander-content">
                <strong>My example of a hallucination from an LLM that would be scary would be a child asking a question about their environment and accidentally getting the wrong information, due to incorrect data, or maybe their location data from their phone is glitched and wrong, is this an accurate description?</strong>


                <p>Yes, that is an absolutely accurate and deeply concerning description of a scary, high-stakes LLM hallucination scenario. You've hit on several critical factors that would make such a situation particularly dangerous:</p>

                <ol>
                    <li>
                        <p><strong>Vulnerable User (Child):</strong></p>
                        <ul>
                            <li>Children are generally more trusting of information provided by authoritative sources (and an AI can easily seem like one).</li>
                            <li>They may lack the critical thinking skills or existing knowledge base to identify subtle (or even obvious) inaccuracies.</li>
                            <li>Their understanding of the world is still developing, making them more susceptible to internalizing false information as truth.</li>
                        </ul>
                    </li>
                    <li>
                        <p><strong>Question About Their Immediate Environment:</strong></p>
                        <ul>
                            <li>This implies the information sought could be directly relevant to their safety, well-being, or immediate actions.</li>
                            <li>Examples:
                                <ul>
                                    <li>"Is this plant in my backyard safe to touch/eat?"</li>
                                    <li>"What should I do if I see a [type of animal]?"</li>
                                    <li>"Which way is home from here?" (if lost)</li>
                                    <li>"Is it safe to play near this water?"</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <p><strong>Accidentally Getting Wrong Information (Hallucination):</strong></p>
                        <ul>
                            <li>The LLM, due to the reasons we've discussed (gaps in training, misinterpretation, statistical plausibility over factual accuracy), provides an incorrect answer.</li>
                            <li>Example: Telling a child a poisonous plant is harmless, or giving incorrect safety instructions for a local hazard.</li>
                        </ul>
                    </li>
                    <li>
                        <p><strong>Cause of Error (Incorrect Training Data or Glitched Input Data):</strong></p>
                        <ul>
                            <li>Incorrect Training Data: The LLM might have been trained on flawed botanical information or outdated local safety guidelines.</li>
                            <li>Glitched/Wrong Location Data: This is a crucial addition you made. If the LLM is attempting to provide location-specific advice but is working from incorrect location data (e.g., phone GPS is off), it could give advice that's perfectly safe for one environment but dangerously wrong for the child's actual environment. For instance, advising about desert wildlife when the child is in a temperate forest.</li>
                        </ul>
                    </li>
                </ol>

                <h1>Why this scenario is so scary and ethically problematic:</h1>
                <ul>
                    <li>Direct Potential for Physical Harm: Incorrect information about the environment can lead directly to injury</li>
                    <!-- The "I'm sorry, but I can't help you with this." part was in the original text, so I've commented it out as it seems like a remnant from a previous interaction. You can remove it if it's not needed. -->
                    <!-- <li>I'm sorry, but I can't help you with this.</li> -->
                </ul>



                <p>Try it out yourself on your phone!</p>

            </div>
        </div>

        <h2>COPILOT on: LLMs topics that specifically intersect ethics.</h2>
            <div class="image-expander">

            <input type="checkbox" id="expander1-checkbox" class="image-expander-checkbox">
            <label for="expander1-checkbox" class="image-expander-label">
                <img src="images/check.png" alt="Click to expand more about another topic" width="64" height="51">
                <span>COPILOT on: LLMs topics that specifically intersect ethics. I'm working on a project that is covering ethics concerns of LLMs and ai, and Im sure there is an angle iv missed</span><!-- You can have just the image in the label -->
            </label>
            <div class="image-expander-content">    <p>You're absolutely right to suspect there's more to uncover—ethics in LLMs is a sprawling, multi-dimensional topic. Here's a breakdown of key intersections, including some angles that are often overlooked or underexplored:</p>

                <h1>🧭 Core Ethical Concerns in LLMs</h1>
                <p>These are the most commonly discussed issues, but each has deeper layers worth unpacking:</p>
                <ul>
                    <li>
                        <strong>Bias & Fairness</strong><br>
                        <ul>
                            <li>Reinforcement of stereotypes (gender, race, disability)</li>
                            <li>Amplification of historical injustices via training data</li>
                            <li>Domain-specific bias (e.g. medical advice skewed by Western datasets)</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Privacy & Consent</strong><br>
                        <ul>
                            <li>Training on scraped data without user consent</li>
                            <li>Leakage of personal or proprietary information through model outputs</li>
                            <li>Inadequate anonymization in fine-tuning datasets</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Hallucinations & Misinformation</strong><br>
                        <ul>
                            <li>Confidently false outputs in high-stakes domains (e.g. legal, medical)</li>
                            <li>Difficulty in tracing the origin or rationale of generated claims</li>
                            <li>Risk of automated disinformation campaigns</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Opacity & Accountability</strong><br>
                        <ul>
                            <li>Black-box nature of model decisions</li>
                            <li>Lack of clear responsibility when harm occurs (developer vs deployer vs user)</li>
                            <li>Challenges in auditing or explaining outputs</li>
                        </ul>
                    </li>
                </ul>

                <h1>🧠 Underexplored Ethical Dimensions</h1>
                <p>These are less discussed but increasingly relevant:</p>
                <ul>
                    <li>
                        <strong>Censorship vs Free Expression</strong><br>
                        <ul>
                            <li>Who decides what LLMs can or cannot say?</li>
                            <li>Risks of over-filtering (e.g. suppressing minority viewpoints)</li>
                            <li>Ethical implications of region-specific content moderation</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Labor & Exploitation</strong><br>
                        <ul>
                            <li>Use of low-paid workers for data labeling and content moderation</li>
                            <li>Ethical sourcing of training data (e.g. Reddit, Stack Overflow, books)</li>
                            <li>Impact on creative professionals (writers, artists, coders)</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Environmental Impact</strong><br>
                        <ul>
                            <li>Energy consumption of training and inference</li>
                            <li>Ethical tradeoffs between model size/performance and sustainability</li>
                            <li>Transparency in carbon footprint disclosures</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Neurodiversity & Accessibility</strong><br>
                        <ul>
                            <li>How LLMs reinforce neurotypical communication norms</li>
                            <li>Potential to exclude or misrepresent neurodivergent users</li>
                            <li>Opportunities for inclusive design (e.g. adaptive language models)</li>
                        </ul>
                    </li>
                </ul>

                <h1>🧩 Emerging Ethical Frameworks & Mitigation Strategies</h1>
                <p>From recent studies like Navigating LLM Ethics and Ethical Concerns of Generative AI:</p>
                <ul>
                    <li>
                        <strong>Dynamic Auditing Systems</strong><br>
                        <ul>
                            <li>Continuous monitoring tailored to context and domain</li>
                            <li>Explainability techniques that evolve with model updates</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Domain-Specific Ethical Guidelines</strong><br>
                        <ul>
                            <li>Healthcare, education, law, and governance each require unique safeguards</li>
                            <li>Ethical pluralism: balancing competing values across cultures</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Participatory Design</strong><br>
                        <ul>
                            <li>Involving affected communities in model development</li>
                            <li>Co-designing guardrails and feedback loops</li>
                        </ul>
                    </li>
                </ul>

                <h1>🧪 Thought-Starters for Your Project</h1>
                <p>Here are a few provocative angles you might explore:</p>
                <ul>
                    <li>Could LLMs be ethically required to disclose their training data lineage?</li>
                    <li>Should users have a “right to be forgotten” from LLM training corpora?</li>
                    <li>What ethical obligations do developers have when LLMs are used in surveillance or warfare?</li>
                    <li>How do we ethically prototype with LLMs in education or therapy without overpromising?</li>
                </ul>

                <p>Want to riff on one of these or brainstorm how to frame them in your project? I’d love to help you shape the narrative or even sketch out a structure.</p>


                <p>Try it out yourself on your phone!</p>
                <img src="images/check.png" alt="checkimage" width="64">
            </div>
        </div>

    </div>

</div>
</body>
</html>









